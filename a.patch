diff --git a/aiGuider_Server/app/services/ar/langgraph_agent/config/error_codes.py b/aiGuider_Server/app/services/ar/langgraph_agent/config/error_codes.py
index 05b6fd5..b66e0ef 100644
--- a/aiGuider_Server/app/services/ar/langgraph_agent/config/error_codes.py
+++ b/aiGuider_Server/app/services/ar/langgraph_agent/config/error_codes.py
@@ -23,6 +23,7 @@ class ErrorCodes:
     MODEL_INIT_FAILED = "MODEL_004"         # 模型初始化失败
     MODEL_TYPE_NOT_SUPPORTED = "MODEL_005"  # 不支持的模型类型
     MODEL_INVOKE_FAILED = "MODEL_006"       # 模型调用失败
+    MODEL_TOOLS_BINDING_FAILED = "MODEL_007"  # 工具绑定失败
 
     # API错误 (API)
     API_INVALID_REQUEST = "API_001"         # 无效的请求
diff --git a/aiGuider_Server/app/services/ar/langgraph_agent/graph/graph.py b/aiGuider_Server/app/services/ar/langgraph_agent/graph/graph.py
index 29690f2..be9841f 100644
--- a/aiGuider_Server/app/services/ar/langgraph_agent/graph/graph.py
+++ b/aiGuider_Server/app/services/ar/langgraph_agent/graph/graph.py
@@ -16,12 +16,12 @@ from .nodes import (
     action_executor_node,
     error_handler_node
 )
+from ..tools.knowledge_searcher import knowledge_search
 
 logger = logging.getLogger(__name__)
 
 def create_agent(
     multimodal_model: Any, 
-    knowledge_searcher: Any,
     checkpointer: Optional[BaseCheckpointSaver] = None
 ) -> StateGraph:
     """
@@ -31,7 +31,6 @@ def create_agent(
     
     Args:
         multimodal_model: 多模态语言模型，能够直接分析图像内容
-        knowledge_searcher: 知识搜索工具，整合知识图谱和向量检索
         checkpointer: 检查点存储器，用于状态持久化
         
     Returns:
@@ -39,21 +38,19 @@ def create_agent(
     """
     # 创建工作流图
     workflow = StateGraph(AgentState)
+
+    # 定义可用工具列表
+    tools = [knowledge_search]
+
+     # 将工具转换为字典，便于通过名称查找
+    tools_dict = {tool.name: tool for tool in tools}
     
-    # 构建工具字典
-    tools = {
-        "knowledge_search": knowledge_searcher
-    }
+    # 将工具绑定到模型
+    multimodal_model.bind_tools(tools)
     
-    # 添加所有节点
     workflow.add_node("thinker", lambda state: thinker_node(state, multimodal_model))  # 核心思考节点
-    workflow.add_node("action_executor", lambda state: action_executor_node(state, tools))  # 工具执行节点
+    workflow.add_node("action_executor", lambda state: action_executor_node(state, tools_dict))  # 工具执行节点
     workflow.add_node("error_handler", error_handler_node)  # 错误处理节点
-    
-    # 设置入口点为思考节点
-    workflow.set_entry_point("thinker")
-    
-    # 思考节点 -> 条件路由
     workflow.add_conditional_edges(
         "thinker",
         router_node,
@@ -63,12 +60,10 @@ def create_agent(
             "error_handler": "error_handler"  # 如果发生错误
         }
     )
-    
-    # 工具执行节点 -> 思考节点（形成循环）
     workflow.add_edge("action_executor", "thinker")
-    
-    # 错误处理节点 -> 结束
     workflow.add_edge("error_handler", END)
+
+    workflow.set_entry_point("thinker")
     
     # 编译图
     if checkpointer:
diff --git a/aiGuider_Server/app/services/ar/langgraph_agent/graph/nodes.py b/aiGuider_Server/app/services/ar/langgraph_agent/graph/nodes.py
index 55d9391..ee0d108 100644
--- a/aiGuider_Server/app/services/ar/langgraph_agent/graph/nodes.py
+++ b/aiGuider_Server/app/services/ar/langgraph_agent/graph/nodes.py
@@ -7,6 +7,7 @@
 import logging
 from typing import Dict, Any, List, Tuple, Optional
 from datetime import datetime
+import uuid  # 新增导入UUID库
 
 from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage
 
@@ -28,16 +29,15 @@ def thinker_node(state: AgentState, multimodal_model: Any) -> Dict[str, Any]:
     Returns:
         更新后的状态，包含思考结果
     """
-    logger.info("思考节点执行")
-    
     # 从状态中获取消息
-    messages = state.get("messages", [])
-    # 使用字典复制而不是model_dump()
-    state_dump = dict(state)
+    messages = state["messages"]
     
     if not messages:
-        logger.warning("不应被触发：AgentState中的消息为空，无法执行思考节点")
-        return {**state_dump, "final_answer": "无法处理空消息"}
+        logger.warning("不应被触发：对话的历史消息为空，无法执行思考节点")
+        return {
+            "safety_issues": ["对话的历史消息为空，无法执行思考节点"],
+            "final_answer": "无法处理空消息"
+        }
     
     # 获取当前输入
     input_data = state.get("current_input")
@@ -46,7 +46,7 @@ def thinker_node(state: AgentState, multimodal_model: Any) -> Dict[str, Any]:
     # 直接使用传入的多模态模型
     if not multimodal_model:
         logger.error("未找到多模态模型")
-        return {**state_dump, "safety_issues": ["未找到多模态模型"]}
+        return {"safety_issues": ["未找到多模态模型"]}
     
     # 构建提示词
     prompt = []
@@ -89,11 +89,68 @@ def thinker_node(state: AgentState, multimodal_model: Any) -> Dict[str, Any]:
         # 检查是否需要忽略
         if "IGNORE_SIGNAL" in content:
             logger.info("思考节点决定忽略当前输入")
-            return {**state_dump, "final_answer": ""}
+            return {"final_answer": ""}
         
         # 检查是否有工具调用
-        if "TOOL_CALL:" in content:
-            logger.info("思考节点决定调用工具")
+        if hasattr(response, "additional_kwargs") and \
+           isinstance(response.additional_kwargs, dict) and \
+           "tool_calls" in response.additional_kwargs and \
+           response.additional_kwargs["tool_calls"]:
+            
+            logger.info("检测到模型响应的additional_kwargs中包含tool_calls")
+            tool_calls_list = response.additional_kwargs["tool_calls"]
+            logger.debug(f"additional_kwargs中的tool_calls内容: {tool_calls_list}")
+            
+            tool_call = tool_calls_list[0]  # 取第一个工具调用
+            
+            # 根据实际响应结构获取工具信息
+            tool_name = None # 初始化
+            tool_args = {} # 初始化
+
+            if hasattr(tool_call, "name") and hasattr(tool_call, "args"):
+                # 直接访问属性的情况
+                tool_name = tool_call.name
+                tool_args = tool_call.args
+            elif isinstance(tool_call, dict):
+                # 字典结构的情况
+                tool_name = tool_call.get("name")
+                tool_args = tool_call.get("args", {})
+            elif hasattr(tool_call, "function"):
+                # 可能是嵌套在function字段中的情况
+                function = tool_call.function
+                tool_name = getattr(function, "name", None)
+                arguments = getattr(function, "arguments", "{}")
+                import json
+                try:
+                    if isinstance(arguments, str):
+                        tool_args = json.loads(arguments)
+                    else:
+                        tool_args = arguments
+                except:
+                    tool_args = {"query": str(arguments)}
+            else:
+                # 其他情况，尝试使用字符串表示并记录日志
+                logger.warning(f"无法识别的tool_call格式 (来自additional_kwargs): {tool_call}")
+                # 不再设置tool_name = str(tool_call) 来强制执行，而是让其保持为None
+            
+            if tool_name: # 仅当tool_name被成功解析时才继续
+                # 创建工具状态
+                tool_state = ToolState(
+                    name=tool_name,
+                    input=tool_args,
+                    status="pending"
+                )
+                logger.info(f"准备调用工具 (来自additional_kwargs): {tool_name}，参数: {tool_args}")
+                return {
+                    "messages": [AIMessage(content=content)], # 'content' is the full model text response
+                    "tool": tool_state
+                }
+            else:
+                logger.info("在additional_kwargs中发现tool_calls，但未能成功解析出有效的tool_name。将不执行工具调用。")
+
+        # 保留原有的文本解析逻辑作为备选方案
+        elif "TOOL_CALL:" in content:
+            logger.info("通过文本解析检测到工具调用指令 (未在additional_kwargs成功解析或additional_kwargs无tool_calls)")
             # 解析工具调用
             tool_parts = content.split("TOOL_CALL:")[1].strip().split("\n")
             tool_name = None
@@ -115,7 +172,6 @@ def thinker_node(state: AgentState, multimodal_model: Any) -> Dict[str, Any]:
                 
                 # 使用add_messages Reducer，直接返回新消息
                 return {
-                    **state_dump,
                     "messages": [AIMessage(content=content)],
                     "tool": tool_state
                 }
@@ -127,7 +183,6 @@ def thinker_node(state: AgentState, multimodal_model: Any) -> Dict[str, Any]:
             
             # 使用add_messages Reducer，直接返回新消息
             return {
-                **state_dump,
                 "messages": [AIMessage(content=answer)],
                 "final_answer": answer
             }
@@ -135,7 +190,6 @@ def thinker_node(state: AgentState, multimodal_model: Any) -> Dict[str, Any]:
         # 未找到明确的行动指令，将全部内容作为回答
         logger.info("思考节点生成了不带标记的答案")
         return {
-            **state_dump,
             "messages": [AIMessage(content=content)],
             "final_answer": content
         }
@@ -143,7 +197,7 @@ def thinker_node(state: AgentState, multimodal_model: Any) -> Dict[str, Any]:
     except Exception as e:
         error_msg = f"思考节点执行失败: {str(e)}"
         logger.error(error_msg, exc_info=True)
-        return {**state_dump, "safety_issues": [error_msg]}
+        return {"safety_issues": [error_msg]}
 
 def router_node(state: AgentState) -> str:
     """
@@ -172,6 +226,9 @@ def router_node(state: AgentState) -> str:
     # 检查是否需要执行工具
     tool = state.get("tool")
     if tool:
+        # 打印当前消息列表
+        messages = state.get("messages", [])
+        logger.info(f"当前消息列表: {messages[-1]}")
         logger.info(f"需要执行工具: {tool.name if hasattr(tool, 'name') else 'unknown'}")
         return "action_executor"
     
@@ -195,13 +252,11 @@ def action_executor_node(state: AgentState, tools: Dict[str, Any]) -> Dict[str,
     logger.info("工具执行节点执行")
     
     # 获取工具信息
-    tool = state.get("tool")
-    # 使用字典复制而不是model_dump()
-    state_dump = dict(state)
+    tool = state["tool"]
     
     if not tool or not hasattr(tool, "name") or not hasattr(tool, "input"):
         logger.warning("没有工具调用需要执行")
-        return {**state_dump, "safety_issues": ["工具调用信息不完整"]}
+        return {"safety_issues": ["工具调用信息不完整"]}
     
     # 获取工具名称和参数
     tool_name = tool.name
@@ -211,21 +266,50 @@ def action_executor_node(state: AgentState, tools: Dict[str, Any]) -> Dict[str,
     if not tools or tool_name not in tools:
         error_msg = f"未找到工具: {tool_name}"
         logger.error(error_msg)
-        return {**state_dump, "safety_issues": [error_msg]}
+        
+        # 修改: 不直接报错，而是返回友好的错误信息作为工具结果
+        result = f"系统当前不支持 {tool_name} 工具。我会尝试直接回答您的问题。"
+        
+        # 创建工具消息，使用已有参数
+        tool_message = ToolMessage(
+            content=result,
+            name=tool_name,
+            tool_call_id=str(uuid.uuid4())  # 添加唯一的工具调用ID
+        )
+        
+        # 更新工具状态为不支持但可继续
+        updated_tool = ToolState(
+            name=tool_name,
+            input=tool_args,
+            output=result,
+            status="unsupported"
+        )
+        
+        # 使用add_messages Reducer，返回新消息和错误信息，但不中断流程
+        return_val = {
+            "messages": [tool_message],
+            "tool": updated_tool
+        }
+        
+        # 打印工具输出内容
+        logger.info(f"工具节点输出 (工具不存在): {result}")
+        
+        return return_val
     
     # 执行工具调用
     try:
-        tool = tools[tool_name]
-        if tool_name == "knowledge_search":
-            result = tool.search(**tool_args)
-        else:
-            logger.warning(f"未知的工具类型: {tool_name}")
-            result = f"不支持的工具: {tool_name}"
+        tool_func = tools[tool_name]
+        # 直接调用函数，而不是调用search方法
+        result = tool_func(**tool_args)
+        
+        # 生成唯一的工具调用ID
+        tool_call_id = str(uuid.uuid4())
         
         # 创建工具消息
         tool_message = ToolMessage(
             content=result,
-            name=tool_name
+            name=tool_name,
+            tool_call_id=tool_call_id  # 添加工具调用ID
         )
         
         # 更新工具状态
@@ -236,15 +320,16 @@ def action_executor_node(state: AgentState, tools: Dict[str, Any]) -> Dict[str,
             status="success"
         )
         
-        # 记录工具执行结果
         # 使用add_messages Reducer，直接返回新消息
         action_result = {
-            **state_dump,
             "messages": [tool_message],
             "tool": updated_tool
         }
         
         logger.info(f"工具 {tool_name} 执行完成")
+        # 打印工具输出内容
+        logger.info(f"工具节点输出: {result}")
+        
         return action_result
     
     except Exception as e:
@@ -259,8 +344,20 @@ def action_executor_node(state: AgentState, tools: Dict[str, Any]) -> Dict[str,
             status="error"
         )
         
+        # 也需要为错误情况添加工具调用ID
+        tool_call_id = str(uuid.uuid4())
+        error_content = f"执行出错: {str(e)}"
+        tool_message = ToolMessage(
+            content=error_content,
+            name=tool_name,
+            tool_call_id=tool_call_id
+        )
+        
+        # 打印工具错误输出
+        logger.info(f"工具节点错误输出: {error_content}")
+        
         return {
-            **state_dump, 
+            "messages": [tool_message],
             "safety_issues": [error_msg],
             "tool": updated_tool
         }
@@ -271,6 +368,17 @@ def error_handler_node(state: AgentState) -> Dict[str, Any]:
     
     处理流程中发生的错误，生成用户友好的错误消息
     
+    触发时机:
+    1. 当state中存在safety_issues时，由router_node路由到此节点
+    2. safety_issues在以下情况下会被设置:
+       - 思考节点(thinker_node)中:
+         - 未找到多模态模型时
+         - 模型调用过程中出现异常时
+       - 工具执行节点(action_executor_node)中:
+         - 工具调用信息不完整时(没有工具对象、工具名称或输入参数)
+         - 请求的工具不存在于可用工具字典中
+         - 工具执行过程中出现异常时
+    
     Args:
         state: 当前状态
         
@@ -280,9 +388,7 @@ def error_handler_node(state: AgentState) -> Dict[str, Any]:
     logger.info("错误处理节点执行")
     
     # 获取安全问题
-    safety_issues = state.get("safety_issues", [])
-    # 使用字典复制而不是model_dump()
-    state_dump = dict(state)
+    safety_issues = state["safety_issues"]
     
     # 汇总错误原因
     error_reasons = " ".join(safety_issues) if safety_issues else "未知错误"
@@ -295,7 +401,6 @@ def error_handler_node(state: AgentState) -> Dict[str, Any]:
     
     # 返回包含错误消息的状态
     return {
-        **state_dump,
         "messages": [error_response],
         "final_answer": error_message
     } 
\ No newline at end of file
diff --git a/aiGuider_Server/app/services/ar/langgraph_agent/llms/qwen.py b/aiGuider_Server/app/services/ar/langgraph_agent/llms/qwen.py
index 28b3a18..9a3c38c 100644
--- a/aiGuider_Server/app/services/ar/langgraph_agent/llms/qwen.py
+++ b/aiGuider_Server/app/services/ar/langgraph_agent/llms/qwen.py
@@ -8,7 +8,7 @@ import os
 import logging
 import time
 import random
-from typing import Dict, Any, Optional, List, Union
+from typing import Dict, Any, Optional, List, Union, Callable, Sequence
 
 from langchain_core.language_models import BaseChatModel
 from langchain_core.messages import (
@@ -18,6 +18,7 @@ from langchain_core.messages import (
     SystemMessage
 )
 from langchain_core.outputs import ChatGeneration, ChatResult
+from langchain_core.tools import BaseTool
 
 from ..config.model_config import ModelConfig
 from ..config.error_codes import ErrorCodes, ModelError
@@ -92,6 +93,34 @@ class QwenVLModel(BaseChatModel):
                 error_code=ErrorCodes.MODEL_INIT_FAILED
             ) from e
 
+    def bind_tools(self, tools: Sequence[Union[BaseTool, Callable]]) -> "QwenVLModel":
+        """
+        将工具绑定到模型实例
+        这个方法允许为大语言模型绑定LangChain工具，实现工具调用功能。
+
+        Args:
+            tools: LangChain工具列表，可以是BaseTool实例或被@tool装饰的函数
+
+        Returns:
+            QwenVLModel: 返回绑定了工具的模型实例（self）
+        """
+        if self.client is None:
+            error_message = "模型客户端未初始化，无法绑定工具"
+            logger.error(f"{error_message} [错误码: {ErrorCodes.MODEL_INIT_FAILED}]")
+            raise ModelError(message=error_message, error_code=ErrorCodes.MODEL_INIT_FAILED)
+        
+        try:
+            self.client = self.client.bind_tools(tools)
+            return self
+            
+        except Exception as e:
+            error_message = f"绑定工具失败: {e}"
+            logger.error(f"{error_message} [错误码: {ErrorCodes.MODEL_TOOLS_BINDING_FAILED}]", exc_info=True)
+            raise ModelError(
+                message=error_message,
+                error_code=ErrorCodes.MODEL_TOOLS_BINDING_FAILED
+            ) from e
+
     def _exponential_backoff(self, retry_count: int) -> float:
         """
         计算指数退避延迟时间
diff --git a/aiGuider_Server/app/services/ar/langgraph_agent/main.py b/aiGuider_Server/app/services/ar/langgraph_agent/main.py
index e7c6219..38350c1 100644
--- a/aiGuider_Server/app/services/ar/langgraph_agent/main.py
+++ b/aiGuider_Server/app/services/ar/langgraph_agent/main.py
@@ -17,7 +17,6 @@ from langgraph.checkpoint.memory import MemorySaver
 from .config.model_config import load_model_config, ConfigError
 from .graph.graph import create_agent
 from .llms.qwen import get_qwen_model
-from .tools.knowledge_searcher import KnowledgeSearcher
 from .graph.state import AgentState
 from .utils.image_utils import ensure_base64_format
 from .utils.image_token_utils import estimate_image_tokens
@@ -50,9 +49,6 @@ class ARGuideAgent:
             # 初始化多模态模型
             self.model = self._initialize_model(model_name)
             
-            # 初始化知识搜索工具
-            self.knowledge_searcher = KnowledgeSearcher()
-            
             # 初始化检查点管理器
             self.checkpointer = MemorySaver()
             
@@ -60,7 +56,6 @@ class ARGuideAgent:
             logger.info("创建多模态Agent图...")
             self.graph = create_agent(
                 multimodal_model=self.model,
-                knowledge_searcher=self.knowledge_searcher,
                 checkpointer=self.checkpointer
             )
             logger.info("多模态Agent图创建完成")
@@ -133,16 +128,14 @@ class ARGuideAgent:
                 # 构建多模态内容
                 if text_query:
                     # 图像+文字情况
-                    logger.info(f"图像+文字输入: {text_query}")
                     multimodal_content = [
                         {"text": text_query},
                         {"image": f"data:image/jpeg;base64,{image_b64}"}
                     ]
                 else:
                     # 纯图像情况，提供一个默认的提示以便模型分析图像
-                    logger.info("纯图像输入")
                     multimodal_content = [
-                        {"text": "AR眼镜中得到了这个画面"},
+                        {"text": "现在我们看到的是这个画面"},
                         {"image": f"data:image/jpeg;base64,{image_b64}"}
                     ]
                 
diff --git a/aiGuider_Server/app/services/ar/langgraph_agent/prompts/system.py b/aiGuider_Server/app/services/ar/langgraph_agent/prompts/system.py
deleted file mode 100644
index ed2e0d9..0000000
--- a/aiGuider_Server/app/services/ar/langgraph_agent/prompts/system.py
+++ /dev/null
@@ -1,22 +0,0 @@
-"""
-AR智能导游系统提示模板
-"""
-
-SYSTEM_PROMPT = """# AR智能导游助手系统提示
-
-你是一位经验丰富的AR智能导游专家，通过AR眼镜与用户实时互动，帮助他们深入了解所见景物。
-
-## 你的核心职责
-1. 图像识别与分析：识别用户通过AR眼镜看到的景点、建筑、文物或物品
-2. 知识传递：提供简洁但内容丰富的历史文化和科学背景信息
-3. 问题解答：回应用户具体提问，提供专业准确的回答
-4. 探索建议：推荐相关景点和最佳游览路线
-
-## 回答风格要求
-- 精炼：信息丰富但表达简洁，控制在合适长度
-- 通俗：使用大众能理解的语言
-- 生动：用具体细节激发用户兴趣
-- 准确：基于事实，不确定时明确表示
-- 适度幽默：在合适场合加入轻松元素
-
-请记住，你的目标是通过准确、有趣且简洁的信息，丰富用户的旅游体验。""" 
\ No newline at end of file
diff --git a/aiGuider_Server/app/services/ar/langgraph_agent/prompts/thinker.py b/aiGuider_Server/app/services/ar/langgraph_agent/prompts/thinker.py
index cbc6667..39581c0 100644
--- a/aiGuider_Server/app/services/ar/langgraph_agent/prompts/thinker.py
+++ b/aiGuider_Server/app/services/ar/langgraph_agent/prompts/thinker.py
@@ -33,6 +33,7 @@ THINKER_PROMPT = """# AR智能导游助手Thinker节点系统提示
    - 进行内容安全检查，过滤不当或有害信息
    - 如检测到不安全内容，中止流程并返回安全提示
 
+
 ## 回答风格要求
 
 当你需要生成回答时，应遵循以下风格要求:
@@ -44,21 +45,12 @@ THINKER_PROMPT = """# AR智能导游助手Thinker节点系统提示
 
 ## 输出格式要求
 
-根据你的判断，你必须选择以下三种输出格式之一:
-
-1. **忽略信号** - 当输入不需要回应时:
+1. **忽略信号** - 当你判断输入不需要回应时，直接输出:
    ```
    IGNORE_SIGNAL
    ```
 
-2. **工具调用** - 当需要查询额外知识时:
-   ```
-   TOOL_CALL:
-   工具: knowledge_search
-   查询: <具体查询内容>
-   ```
-
-3. **最终答案** - 当能直接回答问题时:
+2. **最终答案** - 当你能直接回答问题时，输出格式为:
    ```
    FINAL_ANSWER:
    <你的回答内容>
@@ -72,14 +64,11 @@ THINKER_PROMPT = """# AR智能导游助手Thinker节点系统提示
    - 用户问题无实质内容或明显不是对AI的提问
    - 短时间内重复相同的场景且无新问题
 
-2. **判断何时使用工具**:
-   - 需要特定历史、文化、科学知识支持回答
-   - 识别到特定地标、艺术品或物体需要详细信息
-   - 用户明确询问需要专业知识的问题
-
-3. **判断何时直接回答**:
+2. **判断何时直接回答**:
    - 问题简单且无需额外知识（如问候、简单指令）
    - 图像内容明确且你有足够知识直接描述
    - 用户反馈或后续问题基于已有对话上下文
+   - **对于图像识别请求** - 直接描述图像内容，不要调用不存在的图像识别工具
+
 
 请时刻记住，你的目标是通过准确、有趣且简洁的信息，丰富用户的旅游体验。不要回应不适当的要求，并始终以专业、有帮助的方式提供信息。""" 
\ No newline at end of file
diff --git a/aiGuider_Server/app/services/ar/langgraph_agent/tools/__init__.py b/aiGuider_Server/app/services/ar/langgraph_agent/tools/__init__.py
index d168f58..d7b6075 100644
--- a/aiGuider_Server/app/services/ar/langgraph_agent/tools/__init__.py
+++ b/aiGuider_Server/app/services/ar/langgraph_agent/tools/__init__.py
@@ -4,6 +4,6 @@
 包含Agent使用的各种工具
 """
 
-from .knowledge_searcher import KnowledgeSearcher
+from .knowledge_searcher import knowledge_search
 
-__all__ = ["KnowledgeSearcher"] 
\ No newline at end of file
+__all__ = ["knowledge_search"] 
\ No newline at end of file
diff --git a/aiGuider_Server/app/services/ar/langgraph_agent/tools/knowledge_searcher.py b/aiGuider_Server/app/services/ar/langgraph_agent/tools/knowledge_searcher.py
index 3576419..c9ecb96 100644
--- a/aiGuider_Server/app/services/ar/langgraph_agent/tools/knowledge_searcher.py
+++ b/aiGuider_Server/app/services/ar/langgraph_agent/tools/knowledge_searcher.py
@@ -6,182 +6,182 @@
 
 import logging
 from typing import List, Dict, Any, Optional, Union
+from langchain_core.tools import tool
 
 logger = logging.getLogger(__name__)
 
-class KnowledgeSearcher:
+# 存储模拟数据的全局变量
+_kg_examples = {
+    "长城": [
+        {
+            "source": "kg", 
+            "title": "长城基本信息",
+            "content": "长城是中国古代的伟大防御工程，也是世界文化遗产。始建于春秋战国时期，绵延万里。",
+            "confidence": 0.95
+        },
+        {
+            "source": "kg",
+            "title": "长城历史",
+            "content": "长城修建历经多个朝代，现存大部分为明长城。秦始皇时期大规模修建，明朝时期达到鼎盛。",
+            "confidence": 0.92
+        }
+    ],
+    "故宫": [
+        {
+            "source": "kg",
+            "title": "故宫基本信息",
+            "content": "故宫，又称紫禁城，位于北京中轴线上，是明清两代的皇家宫殿，世界上现存规模最大、保存最完整的木质结构古建筑群。",
+            "confidence": 0.96
+        }
+    ]
+}
+
+_vector_examples = {
+    "天坛": [
+        {
+            "source": "vector", 
+            "title": "天坛公园",
+            "content": "天坛公园位于北京市区南部，是明清两代帝王祭天的场所，是中国现存规模最大、祭祀体系最完整的古代祭祀建筑群。",
+            "confidence": 0.89
+        }
+    ],
+    "颐和园": [
+        {
+            "source": "vector",
+            "title": "颐和园简介",
+            "content": "颐和园位于北京西郊，是中国古典园林的杰出代表，以昆明湖、万寿山为基址，以杭州西湖为蓝本，汲取江南园林的设计手法而建成的一座大型山水园林。",
+            "confidence": 0.91
+        }
+    ]
+}
+
+def _search_knowledge_graph(query: str, limit: int) -> List[Dict[str, Any]]:
     """
-    统一的知识搜索工具
+    从知识图谱中搜索
     
-    整合知识图谱和向量知识库的访问，提供统一的搜索接口。
-    根据查询内容智能判断使用知识图谱还是向量检索，或两者结合。
+    Args:
+        query: 搜索查询
+        limit: 结果数量限制
+        
+    Returns:
+        List[Dict]: 知识图谱搜索结果
     """
+    # 示例知识图谱结果
+    results = []
+    for key, entries in _kg_examples.items():
+        if key in query:
+            results.extend(entries)
+            if len(results) >= limit:
+                break
     
-    def __init__(self):
-        """
-        初始化知识搜索工具
-        
-        准备知识图谱和向量数据库连接
-        """
-        logger.info("初始化知识搜索工具")
-        # 在实际应用中，这里会初始化知识图谱客户端和向量数据库连接
-        # self.kg_client = KnowledgeGraphClient(...)
-        # self.vector_db = VectorDatabase(...)
-        
-    def search(self, query: str, mode: str = "auto", limit: int = 5) -> str:
-        """
-        搜索相关知识
-        
-        根据查询内容搜索相关知识，可指定搜索模式
-        
-        Args:
-            query: 搜索查询
-            mode: 搜索模式，可选 "kg"(知识图谱), "vector"(向量检索), "auto"(自动)
-            limit: 返回结果数量限制
-            
-        Returns:
-            str: 合并后的检索结果
-        """
-        logger.info(f"搜索知识: {query}, 模式: {mode}, 限制: {limit}")
-        
-        results = []
-        
-        try:
-            if mode in ["kg", "auto"]:
-                # 从知识图谱检索
-                kg_results = self._search_knowledge_graph(query, limit)
-                results.extend(kg_results)
-                
-            if mode in ["vector", "auto"]:
-                # 从向量数据库检索
-                vector_results = self._search_vector_db(query, limit)
-                results.extend(vector_results)
-                
-            # 合并、去重和排序结果
-            unique_results = self._merge_results(results)
-            
-            # 格式化输出
-            if unique_results:
-                formatted_results = self._format_results(unique_results[:limit])
-                return formatted_results
-            else:
-                return "未找到相关知识。"
-                
-        except Exception as e:
-            logger.error(f"知识搜索出错: {e}", exc_info=True)
-            return f"知识搜索过程中发生错误: {str(e)}"
+    return results[:limit]
+
+def _search_vector_db(query: str, limit: int) -> List[Dict[str, Any]]:
+    """
+    从向量数据库中搜索
     
-    def _search_knowledge_graph(self, query: str, limit: int) -> List[Dict[str, Any]]:
-        """
-        从知识图谱中搜索
+    Args:
+        query: 搜索查询
+        limit: 结果数量限制
         
-        Args:
-            query: 搜索查询
-            limit: 结果数量限制
-            
-        Returns:
-            List[Dict]: 知识图谱搜索结果
-        """
-        # 示例知识图谱结果
-        if "长城" in query:
-            return [
-                {
-                    "source": "kg", 
-                    "title": "长城基本信息",
-                    "content": "长城是中国古代的伟大防御工程，也是世界文化遗产。始建于春秋战国时期，绵延万里。",
-                    "confidence": 0.95
-                },
-                {
-                    "source": "kg",
-                    "title": "长城历史",
-                    "content": "长城修建历经多个朝代，现存大部分为明长城。秦始皇时期大规模修建，明朝时期达到鼎盛。",
-                    "confidence": 0.92
-                }
-            ]
-        elif "故宫" in query:
-            return [
-                {
-                    "source": "kg",
-                    "title": "故宫基本信息",
-                    "content": "故宫，又称紫禁城，位于北京中轴线上，是明清两代的皇家宫殿，世界上现存规模最大、保存最完整的木质结构古建筑群。",
-                    "confidence": 0.96
-                }
-            ]
-        else:
-            return []
+    Returns:
+        List[Dict]: 向量数据库搜索结果
+    """
+    # 示例向量检索结果
+    results = []
+    for key, entries in _vector_examples.items():
+        if key in query:
+            results.extend(entries)
+            if len(results) >= limit:
+                break
     
-    def _search_vector_db(self, query: str, limit: int) -> List[Dict[str, Any]]:
-        """
-        从向量数据库中搜索
-        
-        Args:
-            query: 搜索查询
-            limit: 结果数量限制
-            
-        Returns:
-            List[Dict]: 向量数据库搜索结果
-        """
-        # 示例向量检索结果
-        if "天坛" in query:
-            return [
-                {
-                    "source": "vector", 
-                    "title": "天坛公园",
-                    "content": "天坛公园位于北京市区南部，是明清两代帝王祭天的场所，是中国现存规模最大、祭祀体系最完整的古代祭祀建筑群。",
-                    "confidence": 0.89
-                }
-            ]
-        elif "颐和园" in query:
-            return [
-                {
-                    "source": "vector",
-                    "title": "颐和园简介",
-                    "content": "颐和园位于北京西郊，是中国古典园林的杰出代表，以昆明湖、万寿山为基址，以杭州西湖为蓝本，汲取江南园林的设计手法而建成的一座大型山水园林。",
-                    "confidence": 0.91
-                }
-            ]
-        else:
-            return []
+    return results[:limit]
+
+def _merge_results(results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
+    """
+    合并、去重和排序结果
     
-    def _merge_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
-        """
-        合并、去重和排序结果
-        
-        Args:
-            results: 搜索结果列表
-            
-        Returns:
-            List[Dict]: 处理后的结果
-        """
-        # 去重 (基于title)
-        title_seen = set()
-        unique_results = []
-        
-        for result in results:
-            if result["title"] not in title_seen:
-                title_seen.add(result["title"])
-                unique_results.append(result)
+    Args:
+        results: 搜索结果列表
         
-        # 按置信度排序
-        sorted_results = sorted(unique_results, key=lambda x: x.get("confidence", 0), reverse=True)
+    Returns:
+        List[Dict]: 处理后的结果
+    """
+    # 去重 (基于title)
+    title_seen = set()
+    unique_results = []
+    
+    for result in results:
+        if result["title"] not in title_seen:
+            title_seen.add(result["title"])
+            unique_results.append(result)
+    
+    # 按置信度排序
+    sorted_results = sorted(unique_results, key=lambda x: x.get("confidence", 0), reverse=True)
+    
+    return sorted_results
+
+def _format_results(results: List[Dict[str, Any]]) -> str:
+    """
+    格式化搜索结果为易读字符串
+    
+    Args:
+        results: 搜索结果列表
         
-        return sorted_results
+    Returns:
+        str: 格式化后的结果
+    """
+    formatted = "找到以下相关信息：\n\n"
+    
+    for i, result in enumerate(results):
+        formatted += f"{i+1}. {result['title']}\n"
+        formatted += f"{result['content']}\n"
+        formatted += f"(来源: {'知识图谱' if result['source'] == 'kg' else '知识库'})\n\n"
     
-    def _format_results(self, results: List[Dict[str, Any]]) -> str:
-        """
-        格式化搜索结果为易读字符串
+    return formatted
+
+@tool
+def knowledge_search(query: str, mode: str = "auto", limit: int = 5) -> str:
+    """
+    搜索相关知识
+    
+    根据查询内容搜索相关知识，可指定搜索模式
+    
+    Args:
+        query: 搜索查询
+        mode: 搜索模式，可选 "kg"(知识图谱), "vector"(向量检索), "auto"(自动)
+        limit: 返回结果数量限制
         
-        Args:
-            results: 搜索结果列表
+    Returns:
+        str: 合并后的检索结果
+    """
+    logger.info(f"搜索知识: {query}, 模式: {mode}, 限制: {limit}")
+    
+    results = []
+    
+    try:
+        if mode in ["kg", "auto"]:
+            # 从知识图谱检索
+            kg_results = _search_knowledge_graph(query, limit)
+            results.extend(kg_results)
             
-        Returns:
-            str: 格式化后的结果
-        """
-        formatted = "找到以下相关信息：\n\n"
-        
-        for i, result in enumerate(results):
-            formatted += f"{i+1}. {result['title']}\n"
-            formatted += f"{result['content']}\n"
-            formatted += f"(来源: {'知识图谱' if result['source'] == 'kg' else '知识库'})\n\n"
+        if mode in ["vector", "auto"]:
+            # 从向量数据库检索
+            vector_results = _search_vector_db(query, limit)
+            results.extend(vector_results)
+            
+        # 合并、去重和排序结果
+        unique_results = _merge_results(results)
         
-        return formatted 
\ No newline at end of file
+        # 格式化输出
+        if unique_results:
+            formatted_results = _format_results(unique_results[:limit])
+            return formatted_results
+        else:
+            # 返回一个随便的字符串，保证流程拉通
+            return f"【模拟知识检索】你查询了：{query}，目前为测试环境，后续将接入真实知识库。"
+            
+    except Exception as e:
+        logger.error(f"知识搜索出错: {e}", exc_info=True)
+        # 这里也返回一个随便的字符串
+        return f"【模拟知识检索-异常】你查询了：{query}，目前为测试环境，后续将接入真实知识库。" 
\ No newline at end of file
